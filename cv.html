<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />


<link rel="stylesheet" href="assets/css/jemdoc.css" type="text/css" />


<link rel="shortcut icon" href="favicon.ico" />
<link rel="bookmark" href="favicon.ico" type="favicon.ico"　/>
<title>QiaoLing Chen (陈巧玲)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-content">
<div id="toptitle">
<h1>QiaoLing Chen (陈巧玲)</h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://chenqll.github.io/"><img src="assets/img/LittleWhite.jpeg" alt="alt text" width="106px" height="150px" /></a>&nbsp;</td>
<td align="left"><br />
Master in NUS<br />
Singapore <br /> 
E-mail: <a href="qiaoling_chen@u.nus.edu">qiaoling_chen@u.nus.edu</a></p>
</td></tr></table>
<h2>About me</h2>
<p>I am currently a Master at NUS and will graduate in November 2023. <br/>

After that, I will be working as an RA in S-Lab Lab at Nanyang Technological University, Singapore, doing research on distributed training and inference for large models.</p>
<h2>Publications</h2>

    <h4 style="color: #e37222;">[Preprint]</h4>
    <p style="font-weight: bold">AMSP: Super-Scaling LLM Training via Advanced Model States Partitioning</p>
        <p>Qiaoling Chen, <a href="https://tonyhao.xyz/">Qinghao Hu</a>, Zhisheng Ye, Guoteng Wang, Peng Sun, <a href="https://personal.ntu.edu.sg/ygwen/">Yonggang Wen</a>, <a href="https://personal.ntu.edu.sg/tianwei.zhang/">Tianwei Zhang</a></p>
        [<a href="https://arxiv.org/abs/2311.00257">pdf</a>][<a href="https://github.com/S-Lab-System-Group/Hydro">code</a>]<a>[Submitted to a Conference]</a><br /><br>

    <h4 style="color: #e37222;">[OSDI 23]</h4>
    <p style="font-weight: bold">Hydro: Surrogate-Based Hyperparameter Tuning Service in Datacenters</p>
        <p><a href="https://tonyhao.xyz/">Qinghao Hu</a>, Zhisheng Ye*, Meng Zhang*, Qiaoling Chen*, Peng Sun, <a href="https://personal.ntu.edu.sg/ygwen/">Yonggang Wen</a>, <a href="https://personal.ntu.edu.sg/tianwei.zhang/">Tianwei Zhang</a></p>
        [<a href="https://www.usenix.org/conference/osdi23/presentation/hu">pdf</a>][<a href="https://github.com/S-Lab-System-Group/Hydro">code</a>]<a>[Artifact Badges: Available Functional Reproduced]</a><br /><br>

    <h4 style="color: #e37222;">[ICPR 22]</h4>
    <p style="font-weight: bold">Feature Transformation for Cross-domain Few-shot Remote Sensing Scene Classification<br />
        <p> Qiaoling Chen*, Zhihao Chen, Wei Luo</p>
        [<a href="https://arxiv.org/abs/2203.02270">pdf</a>][<a href="https://github.com/chenqll/FTM">code</a>]
    </p>


<h2>Projects</h2>
<ol>

<li><p>Long sequence LLM parallel training</p></li>
<ul>
<li><p>Realizing parallel training of LLM with long sequences</p>
</li>
</ul>

<li><p>Super-Scaling LLM Training via Advanced Model States Partitioning, 07.2023-11.2023</p></li>
<ul>
<li><p>undertakes a granular partitioning of model states, encompassing parameters (P), gradient (G), and optimizer states (OS)</p>
</li>

<li><p>builds a unified partitioning space, enabling independent partitioning strategies for P, G, and OS</p>
</li>

<li><p>incorporates a scale-aware partitioner to autonomously search for optimal partitioning strategies</p>
</li>

<li><p>designs a dedicated communication optimizer to ensure proficient management of data placement discrepancies arising from diverse partitioning strategies</p>
</li> 

<li><p>achieves up to 90.3% scaling efficiency across 1024 GPUs</p>
</li> 
</ul>

<li><p>Tuning the cluster resources using the bubble in the data center, 08.2022-12.2022</p></li>
<ul>
<li><p>tuning the cluster resources using the bubble in the data center，achieving 200% optimization.</p>
</li> 
<li><a href="ttps://github.com/Chenqll/od_execution">Using hooks, wrappers, and making small model training on and off by sending signals
    through large models.</a>
</li>
<li><p>Automatic insertion of small models into the training bubble during GPT training without affecting GPT training
    performance, enabling parallel optimization.
    </p>
</li>
</ul>
<li><p>Feature Transformation for Cross-domain Few-shot Remote Sensing Scene Classification
    , 09.2020-8.2022</p></li>
<ul>
<li><p>Designing FTM models and comparing their performance with classical algorithms.
</p>
</li>
<li><p>Validate the effectiveness of the model on remote sensing scene classification tasks on cross-domain few-sample
    scenarios. The method is trained on different sample sizes from 3 - 50, and the results show an average
    improvement of 8.1% in recognition accuracy over model fine-tuning and fine-tuning of the BN layer.
    </p>
</li>
<li><p>Validating the applicability of the model to the land cover classification task on a cross-domain, small-sample
    scenario, the FTM model improves the F1 score by 16% over the model-trimmed, fine-tuned BN layer.
    </p>
</li>
<li><p>Design of a "seed point-SLIC" method that is more efficient than "winner-take-all" image segmentation classification
    methods.
    </p>
</li>
</li>
</ul>

</ol>


<h2>working experience </h2>
<p> Intern at ShangHai AI Lab</p>
<ol>
<li><p> Worked on distributed training development for a 7 billion parameter base model and a chat model<a href="https://github.com/InternLM/InternLM"> InternLM</a>. Enabling linear acceleration of large models within 1024+ ranks</p></li>
</ol>

<p>Algorithm Engineer Intern at Oneflow</p>
<ol>
<li><p> Development and optimization of the OneFlow distributed deep learning framework operator library and
    deployment of common AI algorithms on the OneFlow Smart Cloud. 03.2022-09.2022</p></li>
<ul>
<li><p>OneFlow - libai-chat: <a style="color:#e37222">RLHF-PPO</a> algorithm implementation</p></li>
<li><p>OneFlow <a style="color:#e37222">RWKV</a>-AI-writer implementation, including loss-alignment, parallel optimization, final project outperforms the
    original in terms of memory and throughput for single card, data parallelism, and model parallelism.</p>
</li>
<li><p>OneFlow - OCR algorithm implementation ,optimization and cloud deployment using CRNN, CTPN </p>
</li>

<li><p>OneFlow BERT-Based-NER development and implementation</p>
</li>
<li><p>OneFlow distributed framework operator error message optimisation and test development</p>
</li>

</ul>
</ol>




<h2>Education</h2>
<p>M.E., Enterprise Biz Analytics,National University of Singapore, 08.2022- 09.2023</p>

<p>B.E., Information System, 06.2022</p>
<ul>
<li><p>Main Courses: Advanced Mathematics 98, Linear Algebra 91, Probability and Mathematical Statistics 94, Data Structures 91, Intelligent
    Decision Making 95, Data Analysis and Visualisation 90, Big Data Management and Applications 97</p>
</li>
</ul>

<ul>
    <li><p>GPA：91.61/100 Rank：5 / 120</p>
</li>
</ul>
<p><br />
</td>
</tr>
</table>
</body>
</html>